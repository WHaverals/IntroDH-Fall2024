# Integrity and Ethics in Digital Humanities

## Ethical considerations

All great humanities researchers understand that there are always some ethical aspects to consider when studying human societies and individuals. 

**Social sciences** have a long history of discussing and debating these issues, and there are many resources available to help researchers navigate these issues.

**Informed consent**: In **social science** the human subject is living, for example, the researcher should acquire _informed consent_ from the subject, explaining in clear detail the reason and purpose for the study and how the data from the study will be used (a common word for this is transparency). 

**Privacy**: The researcher(s) should also work to ensure that the _privacy_ of the subject(s) is not being violated. 

**Cultural sensitivity**: The researcher(s) should also demonstrate _cultural sensitivity_ and respect for the subject(s), and a common method for this is explicitly placing the data within it's proper cultural context.

Digital Humanities is increasingly becoming a field that is concerned with these issues, and there are many resources available to help researchers navigate these issues. DH especially is becoming a field of ethical narratives. 

Methodologies and tools in DH are **increasingly aligned** with projects that advocate for change and responsibility. We've seen a bunch of methodologies, tools, techniques but also research questions and projects that are **grounded in ethical narratives** or projects that are **driven by social justice imperatives**.

Just to give you a couple of examples:

- **_Mining the Dispatch_**: topic modeling to uncover discourse around “fugitive slave ads”.
  
    A project that we looked at in the beginning of the semester, which is a project that uses data to visualize the extent of the slave trade in the United States. The project uses data to show the patterns of the slave trade, to show the impact of the slave trade on the community, and to show the failures of the justice system in addressing the problem.

But also in your **data bio's** there were some examples of projects that are grounded in the community and social justice: 

- **_The Survey of Scotish WitchCraft_** putting an ostracized group of people back into the historical record
    
     both Helen, and also Raphi and Emanuelle reported o this, is putting an ostracized group / stigmatized group of people (mostly women) back into the historical record.

- **_Dying inside_**: Also reminds me of the project Alison reported on in her data bio on people dying in US jails. Because there was no public record, Reuters journalists decided to compile and disseminate a dataset that tracks all jail deaths from 2008-2019 from every large jail in America (defined as those with 750+ inmates). 
 
- **_Museum Salary Transparency Salary_** project that Talia reported on. The project uses data to show the patterns of the wage gap, to show the impact of the wage gap on the community.

- Layla reporting on data associated with the Bechdel test and diversity in film.

- The project that Lauren Klein mentions at the end of the presetation on Data Feminism where she and her colleagues Soni Sandeep and Jacob Eisenstein have worked on how abolitionist newspapers played a crucial role in spreading information and shaping public opinion around a range of issues relating to the abolition of slavery.

## The Library of Missing Datasets

All these projects are grounded in ethical narratives or projects that are driven by social justice imperatives. But more thamn that, they all seem to be driven by a sense of _responsibility_ to the data and to the people represented in the data.

Artist and educator Mimi Onuoha's project, The Library of Missing Datasets, is a **critical commentary on absent data** that should exist to address social issues—highlighting the gaps and the data's consequential responsibility.

Ties in to the idea of the _Library of Missing Datasets_ (artwork) by Mimi Onuaha, which is a project that uses data to visualize the extent of the missing datasets in the world. Mimi Onuoha—an artist, designer, and educator—has long been asking who questions about data science. Her project, The Library of Missing Datasets (figure 1.4), is a list of datasets that one might expect to already exist in the world, because they help to address pressing social issues, but that in reality have never been created. The project exists as a website and as an art object.

## Data Against Feminicide

Catherine D’Ignazio's collaboration on **Data Against Feminicide** with Silvana Fumega and Helena Suárez Val focuses on collecting and standardizing data to combat feminicide; a huge blind spot / or a missing dataset. Hard to collect this information, but even if the collected data isn't fully complete, **this project will be successful**, because it demonstrates the potential of DH for activism and social change.

These narratives and projects underscore a duty towards the represented data and subjects.
They demonstrate the power of DH to make an impact beyond the academic sphere.

## Data Feminism

In their book, Data Feminism, Catherine D'Ignazio and Lauren Klein argue that data scientists have a responsibility to use their expertise to ensure that the data is not misinterpreted or misused. 

To guide them in their work, they developed seven core principles. Individually and together, these principles  emerge from the foundation of intersectional feminist thought (which is also something they mention in the video).

> Sidenote on **intersectional feminism**: a framework that identifies how different aspects of social and   political discrimination overlap with gender. It recognizes that women experience layers of oppression caused by race, ethnicity, class, sexuality, age, ability, and other social markers, which compound and influence their experiences of discrimination and privilege.

> The term "intersectionality" was coined by Kimberlé Crenshaw, a legal scholar, in 1989. Her work highlighted that the experiences of black women are shaped by the intersection of sexism and racism, which cannot be fully understood by looking at either in isolation.

> Intersectional feminism, therefore, advocates for an inclusive approach that addresses the varied and sometimes overlapping forms of discrimination that affect women and non-binary people, particularly those from marginalized groups. It seeks to understand and confront the complexity of these experiences in the struggle for gender equality and social justice.

These are the seven principles:

1. **Examine power**. Data feminism begins by analyzing how power operates in the world. 
   
   This means: naming and explaining the forces of power that are so baked into our daily lives--and hence: into our datasets, into our databases, our algorithms--that we often don't see them.
   
   Recognizing that data are not neutral, that data are shaped by the social, political, and economic forces that produce them, and that data can be used to reinforce existing power structures or to challenge them.

   Just last week we saw that baked into the data of what seems to be a very neutral, unharmful dataset, the data of the Historic American Cookbooks, the term 'oriental' is used. This term is a product of the racist, colonial, and imperialist forces that have shaped our world.

   So. Data feminism **begins by examining how power operates in the world today**. 
   
   This consists of asking who questions about data science: 
   Who does the work (and who is pushed out)? Who benefits (and who is neglected or harmed)? Whose priorities get turned into products (and whose are overlooked)? These questions are relevant at the level of individuals and organizations, and are absolutely essential at the level of society. The current answer to most of these questions is “people from dominant groups,” which has resulted in a privilege hazard so acute that it explains the near-daily revelations about another sexist or racist data product or algorithm.

2. **Challenge power**. Data feminism commits to challenging unequal power structures and working toward justice. 
   
   Recognizing that data can be used to challenge power structures, to hold the powerful accountable, to empower the marginalized, and to build a more just and equitable society.

   e.g. **Risk assssment algorithms** are used today many cities in the United States today to inform judgments about the length of a particular prison sentence, the amount of bail that should be set, and even whether bail should be set in the first place. The “risk” in their name has to do with the likelihood of a person detained by the police committing a future crime. 
   
   Risk assessment algorithms produce scores that influence whether a person is sent to jail or set free, effectively altering the course of their life. But risk assessment algorithms, like redlining maps, are neither neutral nor objective. 
   
   In 2016, Julia Angwin led a team at ProPublica to investigate one of the most widely used risk assessment algorithms in the United States, created by the company **Northpointe** (now Equivant). Her team found that white defendants are more often mislabeled as low risk than Black defendants and, conversely, that Black defendants are mislabeled as high risk more often than white defendants.
   
   Digging further into the process, the journalists uncovered a 137-question worksheet that each detainee is required to fill out (figure 2.3). 
   
   The detainee’s answers feed into the software, in which they are compared with other data to determine that person’s risk score. Although the questionnaire does not ask directly about race, it asks questions that, given the structural inequalities embedded in US culture, serve as proxies for race. These include questions like whether you were raised by a single mother, whether you have ever been suspended from school, or whether you have friends or family that have been arrested. In the United States, each of those questions is linked to a set of larger social, cultural, and political—and, more often than not, racial—realities. For instance, it has been demonstrated that 67 percent of Black kids grow up in single-parent households, whereas only 25 percent of white kids do.
   
   Similarly, studies have shown that Black kids are punished more harshly than are white kids for the same minor infractions, starting as early as preschool. So, though the algorithm’s creators claim that they do not consider race, race is embedded into the data they are choosing to employ. What’s more, they are using that information to further disadvantage Black people, whether because of an erroneous belief in the objectivity of their data, or because they remain unmoved by the evidence of how racism is operating through their technology.

   **Sociologist Ruha Benjamin** has a term for these situations: the New Jim Code—where software code and a false sense of objectivity come together to contain and control the lives of Black people, and of other people of color.

   As computer scientist Ben Green states, **“Although most people talk about machine learning’s ability to predict the future, what it really does is predict the past.”** 
   
   Effectively such “predictive” software **reinforces existing demographic divisions**, amplifying the social inequities that have limited certain groups for generations. The danger of the New Jim Code is that **these findings are actively promoted as objective**, 
   
   Benjamin, whose book _Race after Technology: Abolitionist Tools for the New Jim Code_, describes this phenomenon as the **“imagined objectivity of data and technology”** because data-driven systems like redlining and risk assessment algorithms are not really objective at all. Her concept of imagined objectivity emphasizes the role that cultural assumptions and personal preconceptions play in upholding this false belief: one imagines (wrongly) that datasets and algorithms are less partial and less discriminatory than people and thus more "objective".
   
   Assumptions about objectivity are becoming a major focus in data science and related fields as algorithm after algorithm is revealed to be sexist, racist, or otherwise flawed. What can the people who design these computational systems do to avoid these pitfalls? And what can everyone else do to help them and hold them accountable? 
   
   The quest for answers to these questions has prompted the development of a new area of research known as **data ethics**. It represents a growing interdisciplinary effort -both critical and computational— to ensure that the ethical issues brought about by our increasing reliance on data-driven systems are identified and addressed. Thus far, the major trend has been to emphasize the issue of “bias,” and the values of “fairness, accountability, and transparency” in mitigating its effects.
   
   This is a promising development, especially for technical fields that have not historically foregrounded ethical issues, and as funding mechanisms for research on data and ethics proliferate. 
   
   However, as Benjamin’s concept of imagined objectivity helps to show, **addressing bias in a dataset is a tiny technological Band-Aid for a much larger problem**. Even the values mentioned here, which seek to address instances of bias in data-driven systems, are themselves non-neutral, as they locate the source of the bias in individual people and specific design decisions. 
   
   So how might we develop a practice that results in datadriven systems that challenge power at its source? The following chart (table 2.1) introduces an alternate set of orienting concepts for the field: these are the six ideals that we believe should guide data ethics work.

   In the left-hand column, we list some of the major concepts that are currently circulating in conversations about the uses of data and algorithms in public (and private) life. These are a step forward, but they do not go far enough. On the right-hand side, we list adjacent concepts that emerge from a grounding in intersectional feminist activism and critical thought. The gap between these two columns represents a fundamental difference in view of why injustice arises and how it operates in the world. 
   
   The concepts on the left are based on the assumption that injustice arises as a result of flawed individuals or small groups (“bad apples,” “racist cops,” “brogrammers”) or flawed technical systems (“the algorithm/dataset did it”). 
   
   Although flawed individuals and flawed systems certainly exist, they are not the root cause of the problems that occur again and again in data and algorithms.

   The concepts on the left may do good work, but they ultimately keep the roots of the problem in place. In other words, they maintain the current structure of power, even if they don’t intend to, because they let the matrix of domination off the hook. They direct data scientists’ attention toward seeking technological fixes.

   Sometimes those fixes are necessary and important. But as technology scholars Julia Powles and Helen Nissenbaum assert, “Bias is real, but it’s also a captivating diversion.”

   A broader focus on data justice, rather than data ethics alone can help to ensure that past inequities are not distilled into black-boxed algorithms that -- like the risk assessment algortithm -- determine the course of people’s lives in the twenty-first century.
   

3. **Elevate emotion and embodiment**. Data feminism teaches us to value multiple forms of knowledge, including the knowledge that comes from people as living, feeling bodies in the world.

    Data physicalisations, data sculptures, data sonifications, data visualizations that are not just visual, but also tactile, auditory, olfactory, gustatory.
   
    Recognizing that data are not just numbers, that data are not just abstractions. Data are also lived experiences, data are also bodies.

    Persuasion is everywhere. In the 1980s, **philosopher Donna Haraway** was among the first to connect the seeming neutrality and objectivity of data and their visual display to the ideas about distance.
    
    She described data visualization as “the god trick of seeing everything from nowhere.” The view from nowhere—from a distance, from up above, like a god—may be data visualization’s most signature feature. 
    
    It’s also the most ethically complicated to navigate for the ways in which it masks the people, the methods, the questions, and the messiness that lies behind clean lines and geometric shapes. Haraway calls it **the god trick: it’s a trick because it makes the viewer believe that they can see everything, all at once, from an imaginary and impossible standpoint.**

    E.g. It is this “superpower”—the aerial view from no body—that we see put into practice in the map Coming Home to Indigenous Place Names in Canada (figure 3.9a). Margaret Pearce, a cartographer and member of the Citizen Potawatomi Nation, spent fifteen months collecting Indigenous place names from First Nations, Métis, and Inuit peoples. The map depicts the land that is known in a contemporary Anglo-Western context as Canada, but without any of the common colonial orientation points, like the boundaries of the provinces or the locations of major cities like Ottawa, Montréal, and Nova Scotia.

    Coming Home to Indigenous Place Names in Canada leverages the authority of the god’s-eye view to challenge the colonizer’s view, to advocate for a “reseeing” of the land under terms of engagement that recognize Indigenous sovereignty and respect Indigenous homelands. The extent of geographic territory included and the sheer number of names asserts the Indigenous presence as major, originary, and ongoing. This is by design. Pearce intentionally created the map with the same paper size, fold, scale, and projection as the map published by Natural Resources Canada, the country’s geographic authority.51 By replicating these design features, Coming Home proposes an alternative yet equally authoritative conception of national identity.

    And Coming Home to Indigenous Place Names in Canada establishes that the god trick itself can be used to simultaneously engender emotion and challenge injustice. Rather than making universal rules and ratios (think: data-ink) th
   
4. **Rethink binaries and hierarchies**. Data feminism requires us to challenge the gender binary, along with other systems of counting and classification that perpetuate oppression. 

    For the millions of nonbinary people in the world—that is, people who are not either male or female, men or women—the seemingly simple request to “select gender” can be difficult to answer, if it can be answered at all. Yet when creating an online user account, not to mention applying for a national passport, the choice between “male” or “female,” and only “male” or “female,” is almost always the only one.

    What is counted—like being a man or a woman—often becomes the basis for policymaking and resource allocation. By contrast, what is not counted—like being nonbinary—becomes invisible. 

    Sometimes, however, the goal of challenging binary thinking can be constrained by the realities of the field. Visualization designers, for example, do not typically have control over the collection practices of the data they are asked to visualize. They often inherit binary data that they then need to “hack” from within. What might this look like? We might point to the reporters on the Lifestyle Desk of the Telegraph, a British newspaper, who, in March 2018, were considering how to honor International Women’s Day and were struck by the significant gender gap in the United Kingdom in terms
    of education, politics, business, and culture.48 As journalists, they were working with multiple sources of data collected by other agencies, which all came in binary form. But they wanted to ensure that they didn’t further reinforce any gender stereotypes. They paid particular attention to color. One line of designer logic would favor cultural convention for interpretability, like using pink for women and blue for men, but a feminist line would use color choices to hack those same conventions (figure 4.6). Pink and blue is, after all, another hierarchy, and the goal of the Telegraph team members was to mitigate inequality, not reinforce it.

    But the Telegraph journalists could have gone one step further to rethink binaries. They had an opportunity to communicate to the public that gender is not a binary by spelling that out—in the text of the story or in a caption under the graphics or by showing visually that there was no data for nonbinary people. Their colleagues at the Guardian recently adopted this latter strategy in their interactive piece “Does the New Congress Reflect You?” about the 2018 US midterm elections.50 The piece presents three categories: cis male, cis female, and trans + nonbinary. When you click on “trans + nonbinary,” as in figure 4.7, the interactive map displays all of the districts in grey, because “0 people in Congress are like you.” The absence of data becomes an important takeaway, as meaningful as the data themselves.
   
5. **Embrace pluralism**. Data feminism insists that the most complete knowledge comes from synthesizing multiple perspectives, with priority given to local, Indigenous, and experiential ways of knowing.

    This goal reflects a key tenet of feminist thinking, which is the recognition that a multiplicity of voices, rather than one single loud or technical or magical one, results in a more complete picture of the issue at hand. Feminist philosophers like Donna Haraway, who we introduced in chapter 3, prompted a wave of thinkers who have continued to develop the idea that all knowledge is partial, meaning no single person or group can claim an objective view of the capital-T Truth. But embracing pluralism, as this concept is often described today, does not mean that everything is relative, nor does it mean that all truth claims have equal weight. And it most certainly does not mean that feminists do not believe in science. It simply means that when people make knowledge, they do so from a particular standpoint: from a situated, embodied location in the world. More than that, by pooling our standpoints—or positionalitiestogether, we can arrive at a richer and more robust understanding of the world.
    
    So, how do we begin down the path to this deeper understanding in data science? The first step in activating the value of multiple perspectives is to acknowledge the partiality of your own. This means disclosing your project’s methods, your decisions, and—importantly for work that strives to address injustice—your own positionalities. This is called reflexivity, and we modeled this in the introduction to this book. You may have heard the phrase coined by David Weinberger, “transparency is the new objectivity.” We take this to mean that there is a way to build space for transparency plus reflexivity in data science, rather than undertaking projects that purport to be objective (but, as we’ve discussed, never really are). Transparency and reflexivity allow the people involved in any particular project to be explicit about the methods behind their project, as well as their own identities.

    People in journalism have been doing this for some time—at least as it relates to their data and methods. For example, Bloomberg’s interactive visualization “What’s Really Warming the World?” (figure 5.4) walks the reader through a range of common arguments that try to explain away global warming with reasons that don’t have to do with human industry or behavior.35 It’s a compelling piece in terms of content alone, but another interesting thing about it is that it devotes nearly a third of its screen real estate to describing its data and methods.

    https://www.bloomberg.com/graphics/2015-whats-warming-the-world/

    The fifth principle of data feminism is to embrace pluralism in the whole process of working with data, from collection to analysis to communication to decision-making. Embracing pluralism is a feminist strategy for mitigating this risk. It allows both time and space for a range of participants to contribute their knowledge to a data project and to do so at all stages of that project. This means transferring knowledge from experts to communities and explicitly cultivating community solidarity in data work.

    We saw this with the postcards at the Library. 

6. **Consider context**. Data feminism asserts that data are not neutral or objective. They are the products of unequal social relations, and this context is essential for conducting accurate, ethical analysis.
   
   The sixth principle of data feminism is to consider context. The bottom line for numbers is that they cannot speak for themselves. In fact, those of us who work with data must actively prevent numbers from speaking for themselves because when those numbers derive from a data setting influenced by differentials of power, or by misaligned collection incentives (read: pretty much all data settings), and especially when the numbers have to do with human beings or their behavior, then they run the risk not only of being arrogantly grandiose and empirically wrong, but also of doing real harm in their reinforcement of an unjust status quo.

7. **Make labor visible**. The work of data science, like all work in the world, is the work of many hands. Data feminism makes this labor visible so that it can be recognized and valued.

If you work in software development, chances are that you have a GitHub account. As of June 2018, the online code-management platform had over twenty-eight million users worldwide. By allowing users to create web-based repositories of source code (among other forms of content) to which project teams of any size can then contribute, GitHub makes collaborating on a single piece of software or a website or even a book much easier than it has ever been before. Well, easier if you’re a man. A 2016 study found that female GitHub users were less likely to have their contributions accepted if they identified themselves in their user profiles as women. (The study did not consider nonbinary genders.)

Coding is work, as anyone who’s ever programmed anything knows well. But it’s not always work that is easy to see. The same is true for collecting, analyzing, and visualizing data. Unfortunately, however, when releasing a data product to the public, we tend not to credit the many hands who perform this work. We often cite the source of the dataset, and the names of the people who designed and implemented the code and graphic elements. But we rarely dig deeper to discover who created the data in first place, who collected the data and processed them for use, and who else might have labored to make creations possible. 

This is a problem because it means that the work of data science is often invisible, and when work is invisible, it is often undervalued.

To put it more simply, it’s not a coincidence that much of the work that goes into designing a data product—visualization, algorithm, model, app—remains invisible and uncredited. In our capitalist society, we tend to value work that we can see.

Even at resource-rich companies like Amazon and Google, **the work of data entry is profoundly undervalued in proportion to the knowledge it helps to create**. Andrew Norman Wilson’s 2011 documentary Workers Leaving the Googleplex (figure 7.4) exposes how the workers tasked with scanning the books for the Google Books database are hired as a separate but unequal class of employee, with ID cards that restrict their access to most of the Google campus and that prevent them from enjoying the company’s famed employee perks.28 (Evidently, working overtime to preserve the world’s cultural heritage still does not entitle you to a free lunch, let alone a free class on how to cook Pad Kee Mao.)29 Wilson also observes that Google’s book-scanning workers are disproportionately women and people of color.
   
cfr. Lauren Klein's artwork, anatomy of an ai system
 
These principles are not just for data scientists. They are for all of us. They are for all of us who work with data, who use data, who are affected by data. They are for all of us who care about justice, who care about equality, who care about freedom.


------


## Statements

Careful, some of these are thought-provoking or controversial. I don't necessarily agree with all of them, but I think they are interesting to think about.

1. The inherent biases in historical data collection are too pervasive to allow for truly objective modern data applications.

Effective data science must account for intersectionality; overlooking this is not just a methodological flaw, but an ethical failure.

Openness about the algorithms used in data processing is crucial for ensuring they do not perpetuate existing societal biases.

Data scientists have a responsibility to use their skills for social activism, challenging injustices and promoting equity.

Corporations that collect and analyze vast amounts of user data are obligated to use this data to benefit society, not just their shareholders.

technology inherently leads to social progress

The monetization of personal data by corporations should be more heavily regulated by governments to protect individual rights.

Emotional responses and physical experiences should be valued equally with quantitative data in research methodologies.

The future of humanities lies in breaking down the walls between disciplines rather than in further specialization.

Digital tools should extend the capabilities of humanistic study, not merely serve as high-tech replacements for traditional methods.

All digital humanistic work should be openly accessible, to democratize knowledge beyond the traditional academic gates.

Design and architecture should be central to how humanities research questions are formulated and communicated.

The rigid boundaries of traditional humanities disciplines are becoming obsolete in the face of digital convergence and interdisciplinary practices.

Digital tools and platforms should fundamentally reshape not just how we conduct humanities research, but also how we communicate and teach these subjects.

Humanities scholars should embrace 'maker' cultures that incorporate coding, design, and multimedia artistry into their research methodologies.

Curation should be valued as much as traditional scholarship in the humanities.

Researchers must consider not only how they use data ethically but also how their data might be used or misused by others after publication. 

    Is this the role of the researcher?


Curation should be recognized as a fundamental scholarly practice within humanities, equivalent to traditional narrative scholarship."

DH people are so nice because they are not really concerned with theory (more with methodology) and because of that there are fewer "fights" in the field.

## some strong ones

Gatekept knowledge is the enemy of progress. 

Bias is Data Too: "Biases aren't bugs, they're features. Data reflects our choices—change the choices, change the data."

Ethics Aren't Universal: "What's ethical varies by context—universal standards are imperialist.

Dissolve Disciplines: "Interdisciplinary isn't enough; true innovation demands no disciplines.

Scholarship Is Not Sacred: "The sanctity of scholarship is a myth—humanities must innovate or become relics."
Text Is Tyranny: "Prioritizing text marginalizes myriad narratives. Embrace multimedia as the new canon of critical scholarship.

Claiming neutrality in scholarship perpetuates the status quo. Choose a side—silence is complicity.

DH research is inherently political.

DH research tries to find shortcuts to problems that are more complex than can be solved with a simple algorithm. 


dh has the power to visualize and investigate data in ways that other disciplines cannot, it is therefore the moral responsibility of dh scholars to use this power for good and not for evil


rendering the invisible visible: to other people, but also to institutions,

projects grounded in their communities, cultures, and histories. this expertise is necessary to ensure that the data is not misinterpreted or misused.

"we're building tools, collecting data, but we're also building community" (dignazio)

To the best of the one's ability, if people or communities are going to be represented in the data, they should always be consulted and involved in the process.


- https://libguides.franklinpierce.edu/c.php?g=1334337&p=9827008
- https://www.digitalstudies.org/article/id/7331/


