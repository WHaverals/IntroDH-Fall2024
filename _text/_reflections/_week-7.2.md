# Reflections -- Topic Modeling, Part 2

## Readings

  - [Boyd-Graber, Jordan, et al. “Applications of Topic Models.”](https://app.perusall.com/courses/introdh24/boyd-graber-et-al-2017-applications-of-topic-models) _Foundations and Trends in Information Retrieval_, vol. 11, no. 2–3, 2017, pp. 143–296. <small>&rarr; This is a big one! No need to read in depth; become acquainted with chapters 1.1, 1.2 (+ 1.3 if you want to get a bit more technical), chapter 3, and chapter 6.</small>
  - [Antoniak, Maria. _A Computational Reading of a Birth Stories Community_](https://maria-antoniak.github.io//2019/11/04/computational-reading-birth-stories.html). 5 Nov. 2019.

## Pia Bhatia
  11:21 AM
Maria Antoniak’s essay for Thursday was fascinating. It elucidated the ways in which ‘human’ scholars can inform the models they are using—one instance in particular I was thinking about was the way she created an “annotated dictionary of verbs” to explore the power dynamics within these stories. She writes: “Unsurprisingly, the baby is usually framed as having the least amount of power in these stories, while the medical professionals have a lot of power. Perhaps unexpectedly, authors of birth stories often frame themselves as having very little power, while the doula and midwife are framed as having a lot of power. For example, only the doula is framed as having more power than the nurse.”
I can think of so many applications of this kind of research. A professor of mine once explained how a certain pattern has been observed across various biology textbooks where the figure of a pregnant woman is focussed around the fetus. Oftentimes, she said, the woman’s head and face are not included, which depersonalizes her. By sensitizing a software to context-specific details and assumptions, findings like these can change how we perceive the balance of ‘power’ in depictions of birth: for example, Antoniak’s figures of the “relational power” could be incredibly useful as well in highlighting inequities in healthcare.


## Alison Fortenberry
  7:43 PM
Chapter three of the Boyd-Graber reading and the discussion of what interventions are needed to maximize the output of a topic model had me questioning if we are really learning something new when using a topic model. Using the urinary/ Central Nervous System example, if a user has to officially separate topics that they have already identified as separate from prior knowledge, how is the output telling them something new? We already know these topics exist, and the topic model not only regurgitates knowledge we already have, but does so incorrectly on the first try, requiring increased intervention. It seems like we’re adding additional steps for a worse result. I’m not sure that I’ve been fully dissuaded that topic models tell us something we already know, but the Antoniak article helped me to reevaluate whether that is necessarily a bad thing. Antoniak could probably pull out the trends identified by the topic model through a more qualitative reading of birth stories (so the model probably won’t tell her something new), but the presence of a quantitative dataset that supports arguments she may have developed on her own expands the quantity of evidence supporting her argument. It doesn’t tell her something she doesn’t know, but she can use it to analyze a larger volume of information, strengthening the argument grounded in what she already knew.


## Anya Kalogerakos
  10:14 AM
As someone who does not know much statistics, the Boyd-Graber reading occasionally went a bit over my head, but one thing that I found very interesting throughout the reading was the manipulation of topic groups. Similar to the idea of data cleaning, topic grouping without any interference can cause issues with similar topics and use of “functional” rather than “descriptive” words. The two solutions mentioned to these are the use of constraints on topic groups (i.e. “this” cannot be in the same group as “that”) and the use of a stopword list that doesn’t include words listed in the topics. This made me question how the manipulation of topics could yield very different results based on the question a researcher has in mind, which is both a good and bad thing. While there is some worry that you could manipulate your results to get what you want, it is also valuable to see the different possible outcomes from a body of text. The birth stories project was a very interesting example of topic grouping put to work. I think the use of topic groupings to illustrate a chronological process was a brilliant idea, as it added more context to the groupings, while preserving their significance. I wonder if this type of visualization could also help to lessen issues of conflicting topic groups, as it forces the body of text to be split into different, more specific sections before topic groupings are found.


## Andrew Huo
  11:51 AM
After reading parts of Boyd-Graber’s “Applications of Topic Model,” I think I have a much clearer understanding of the concept of topic modeling as well as why it is used especially in the humanities. Chapters one and three helped reinforce the general basis of the process. Essentially, topic models can answer questions from large datasets quickly without human intervention. Chapter three clearly defined the different visualizations and displays that we touched upon in classes and their potential usages, and it was interesting to learn different ways of formally labeling and categorizing data using means such as The Topic Model Visualization Engine, The Topical Guide, and Interactive TOpic Model and MEtadata that evaluated the change of topics over time. But what I found the most interesting was chapter 6: Fiction and Literature which talked about ways that topic modeling can answer questions about works of literature that create their own closed worlds. Topic modeling can help with distant reading, categorizing themes, motifs, and genres – and can, for example unearth themes in works of literature that are hidden in the modern humanities canon. One question I had was in the 6.4 Beyond the Literal section. Boyd-Graber gives an example of how topic modeling can unearth consistent metaphors because “the poets use a consistent “surface” language to represent a consistent metaphor” (Boyd-Graber, 76), but how can one count on the signature of a metaphor? I think there are instances where authors/poets would use different words to describe the same metaphor, and the connection is volatile. Next, what fascinated me was stylometric analysis and how it works opposite to topic modeling by focusing on low-information words and ignoring content-bearing words. This model will unearth the hidden psychology of authors (something I would be fascinated by) and information about the author through hidden patterns and word usage. Lastly, Maria Antoniak’s “A Computational Reading of a Birth Stories Community” was an interesting example of the application of topic modeling and how the process can identify the flow of the narratives of birth stories.


## Clay Glovier
  12:40 PM
I enjoyed reading both Boyd-Graber’s article on Applications of Topic Models and Antoniak’s piece on A Computational Reading of a Birth Stories Community. I’m curious about the author’s statement that the small samples of literary data that are studied through close reading are not randomly selected. Why can one not select a variety of texts that have been closely studied and achieve a normal distribution? If the argument is that many ancient texts have been lost, the prodigious quantities of modern texts make these a small percentage of the overall body of literature. If the argument is that primarily the “great” books through history have been closely studied, I still think there are enough acclaimed works to extrapolate information from in a random fashion. While one would not be selecting from all books ever produced, some books published are not well written, and it is not necessarily the end of the world for all studies if they are not included. I enjoyed reading Antoniak’s article as well, and am curious how the terms used in this particular forum compare with the words used in other groups of recent mothers. Comparing the different terms could offer insight into the many communities of mothers and how they interact with the world throughout the birthing experience. (edited) 


## Talia Goldman
  4:16 PM
In chapter 6 of the Boyd-Graber reading, I was interested in the chunking of fictional texts in comparison to how Antoniak collected Reddit posts into a larger corpus. Boyd-Graber’s explanation of thematic variance through novels was helpful in understanding why a different approach was used for the birth stories project. I had been thinking about topic modeling as, in a way, involving less human intervention than other DH approaches we’ve been talking about. While this is true to an extent, the decisions made in chunking vs. collecting corpuses to be certain sizes, along with number of topics, etc, represent the biases/influential choices that are still present in topic modeling.
I also thought it was interesting that Boyd-Graber writes, “For scholars, these models offer the possibility of a more precise approach to concepts that have traditionally been vague and impressionistic, such as theme, genre, and motif” (Boyd-Graber 77-78). I wonder how exactly Boyd-Graber sees this opportunity for precision playing out, as he notes that paradoxically, the subjectivity of theme leads us to question statistical approaches. From learning about topic modeling, I am further swayed to think that certain elements of the humanities, such as determining theme, are best done with “traditional” humanities methods, but, as was mentioned in the Antoniak article, topic modeling seems to best confirm prior knowledge and research. This could be very helpful in textual analysis, but also seems to present issues of confirmation bias and predetermined interpretations that may misuse data. Perhaps this is another place to consider ethical standards for DH, especially when dealing with large corpuses of  fictional text rather than collected “real world” data.


## James Sowerby
  8:12 PM
I thought it was interesting to look at Antoniak's visualizations of her data and topic models in the context of Boyd-Graber's explanations of the pitfalls of representations in research. Antoniak's article itself was very succinctly written and I appreciated the way that she still didn't fail to acknowledge privacy concerns and even opened herself up to feedback. Even for a personal project like something along these lines, it still seems like a very important responsible thing to do. Boyd-Graber seemed to emphasize in Chapter 3 that representations can often obscure true understanding of a dataset, especially if it wasn't interactive and only displayed certain arbitrary connections between words. Even though Antoniak's topic tree was not interactive, and let some things to be desired, I still thought she did a great job in making a novel point and using her computational methods to real effect. Her assertion that "there is no right way to give birth, and the dataset of stories demonstrates that" was well-taken: the connections between every topic did showed commonalities between all 3,000 of her samples and made it accessible. I will be honest that I didn't go into her full paper and read the research in its expanded form, so I might be missing it. I also wanted to mention how helpful the lead story from the Boyd-Graber article was: the hypothetical situation of the journalist trying to find the smoking gun on Enron really put into context what these applications might be used for, and gave me something to compare the later sections with. Especially skimming through the more mathematical and technical parts, like in 1.3, stories are very useful for me as a learner.


## Pippa LaMacchia
  8:37 PM
Reading Boyd-Graber’s Applications of Topic Modeling next to Antoniak’s clear and concise application of the information in Boyd-Graber’s article was incredibly illuminating. Many of these theoretical articles are challenging to fully apply to an authentic context but I was particularly struck by Antoniak’s research. These past several weeks of discussions and readings have really shown me how humanist researchers can take advantage of and actually use the computational tools that we have at our fingertips. The Boyd-Graber article explains that “one of the reasons for using topic models is that they produce human readable summaries of the themes of large document collections,” but it is also interesting to me that this article takes into consideration visualization and understandability of these sources. The article refers to “human interpretability ratings” which is an essential part of studying this data that I hadn’t fully thought about. I still would like to better understand how these visualizations are created because it is of primary importance that the user themselves can actually interact with the information generated by topic models. The data is useless if it cannot be properly interpreted — a basic and obvious idea but one I hadn’t fully considered in context yet. I am also interested in how this topic modeling changes with fictional and artistic sources as elaborated upon in chapter 6. Boyd-Graber explains that “Topic models cannot by themselves study literature but they are useful tools for scholars studying literature” and I would love to see an example of this work in a scholarly fictional realm.

## Helen Gao
  12:36 AM
Reading about the probability theory behind topic modeling in “Applications of Topic Models” made me think about how, even if humanities scholars understand the math behind the algorithms they use, they probably aren’t going to devote time to developing ways to significantly improve these algorithms for the specific purposes of humanities research in their respective fields. For example, the text pointed out that “for Japanese and modern Chinese we must often rely on pre-processing tools that are themselves potentially unreliable”, and since much of NLP research is conducted in English, this gap seems especially significant for researchers focusing on other languages. I also found it a bit concerning that, when faced with the ‘bad’ topic grouping in which the central nervous system and urinary system terminology was combined, rather than investigating why that happened, the researchers just manually changed the topics and called it ‘interactive topic modeling’, which (like other readings have done) highlights how arbitrary the process can be sometimes. I did like that this reading clarified the status of topic models as tools and the relationship between topic modeling and distant reading: that topic models operationalize distant reading.
I thought the analysis in “A Computational Reading of a Birth Stories Community” was fascinating, and I was impressed by how much it revealed about the experience of childbirth. The investigation into quantifying ‘power’ in these stories was also intriguing, though I was somewhat suspicious of how the “annotated dictionary of verbs” was used. The full paper provides the example of the word ‘broke’ in the sentence “The doctor broke my water” as an example of the doctor having a high ‘power’ score, but I would argue that the word ‘broke’ was simply chosen because it is commonly accepted medical terminology, and not necessarily because it reflects the doctor’s ‘power’ (though I do agree that the doctor has a lot of power over the patient in a childbirth scenario). This demonstrates the importance of using metrics that are tailored to specific contexts, rather than uncritically using existing metrics.


## Melissa Woo
  12:55 AM
It’s interesting to see the proposed applications and illustrated examples of how topic modeling might be most useful in the wild – the author mentions a journalist digging through a trove of documents for a needle or a search for an omitted topic in a vast set of resources. I wonder how the sensitive nature of these applications could shape how topic modeling is used – in either situation, it seems that a robust understanding of how the algorithm/method is working and how the results are created is highly important to have faith and trust in the resulting findings, especially if it were, for example, published in a newspaper or reported to a company’s executive board. Does randomness in topic initiation or other technical complexity obscure the potential applications of this type of digital humanistic technology given the potential barrier for an unfamiliar audience to fully appreciate what is being measured and how?
I wonder also how the prevalence and development of LLMs is going to change the name of the game of topic modeling. It seems that being able to quickly process huge amounts of text and answer questions about general topics covered in the text, but also much more specific and nuanced questions has the potential to render more basic forms of topic modeling obsolete. But that being said, I think that “A Computational Reading of a Birth Stories Community” actually provides a counterargument to this line of thinking; the research process showing how the questions developed and evolved to lead to a particular line of questioning around the power associated with various characters of birth stories (babies, mothers, doulas, etc.) is based on an understanding of the context of these stories and a progressive analysis of topic modeling from the initial flowchart to the latter power score visualizations. It is non obvious to me that ChatGPT or some other type of LLM would be able to produce this type of contextualized research with findings that build on each other without human design and input.


## Colin Brown
  1:06 AM
When reading the Antoniak story, I appreciated how it brought a different perspective to the topic modeling discussions that we’ve had so far in that it was much more personal. In most of the topic modeling we’ve looked at, the subjects are fairly objective: what is the occurrence of certain characters over time? How does the frequency of certain authors being discussed change over time? In contrast, Antoniak’s research was first inspired by her friends being vulnerable, and she chose to look into child birth stories, a certainly very powerful and emotional moment in many people’s lives. By using topic modeling to map the many different birth story experiences online, Antoniak not just describes a literary trend, but provides emotional reassurance and support to people who go through these situations.
In the Boyd-Graber article, I found their haystack analogy to be a helpful visual for topic modeling; in essence, we care about the “bulk” of the content. The analogy, though, continues to prop up an area of weakness for topic modeling, which is its lack of capturing word sequence. For instance, how long is the straw in our haystack? What shape is the haystack? How tightly packed is the haystack? All of these questions could yield insights to our stash of hay on the farm, and similarly, I wonder what comes out of considering sequences in topic modeling? I’m sure this question also runs into the realities of computational power, memory, and data sparsity.


## Layla Williams
  1:19 AM
The lecture in class on Tuesday combined with these readings gave me a greater understanding of how the actual process of topic modeling occurs. I especially think it was interesting to hear the differences between topic modeling and the voyant tools (with one being more of exploration and the other with the possibility of testing a particular hypothesis). However, I might think the division between the tools on their basis of exploration can be widened; this is more in response to the fact that topic modeling does not necessarily ask the computer for a specific set of topics but rather the researcher can explore and discover the topics that they are given. I think based on my interests in the arts, I was interested in the literature section that noted how topic modeling can be used to track certain character arcs and relationships within a particular text. In addition, I thought it was interesting that the machine learning model cannot understand figurative language but instead understands the relationship between words. We can learn about the literal parts of speech that we use associated with these instances of figurative language to see what we tend to associate with them. Finally, I thought the reddit exploration of birth stories was an innovative way of using topic modeling, especially because of how it ended with a suggestion for the health industry to see these narratives and learn to respect them. It demonstrates that tools of the digital humanities can have impacts beyond the humanities and into the social and hard sciences.


## Ethan Haque
  4:09 AM
I think the Antoniak article is cool and interesting, but I'm not sure what they've accomplished with their analysis. I skimmed the full paper in addition to the paraphrased article, and they provide many more details about how they generated their insights and visualizations, but I still felt like important details were missing. Like why they chose certain hyperparameters, error/uncertainty measurements, or evaluation metrics for quality assessment for example. In addition, from the Boyd-Graber paper and earlier scholarship as well, we know that topic modeling techniques are highly sensitive to the tokenization and document boundary selection process. I couldn't find an ablation study in the paper that describes how their analysis is robust to varying certain parameters like the number of topics in the LDA model nor how different boundary conditions might change topics, so I'm not convinced their results aren't capturing tons of noise in their input data. Furthermore, I wonder who is posting the stories they used in their analysis. This subreddit is a very specific corner of the internet so their results inherently lack generalizability, and they throw away information like upvotes and interactions in their analysis, which are like the currency of Reddit and I'd argue one of the main reason people are interested in posting their stories to begin with. The techniques they use and their results are cool, but not sure if they're useful. Does it matter if they're useful? I guess not really, but I doubt the authors are going to claim their analysis doesn't have broader impact on their field.


## Yaashree Himatsingka
  5:22 AM
I think the promise of topic modeling lies in its ability to uncover areas of inquiry which may have otherwise been missed, and which scholars can further extend through analysis. This quote from “Mining the Dispatch” resonated in particular: “Topic modeling and other distant reading methods are most valuable not when they allow us to see patterns that we can easily explain but when they reveal patterns that we can’t, patterns that surprise us and that prompt interesting and useful research questions.” It supports Froehlich’s point that while computers are good at finding patterns, we do need people to interpret them. I wonder if this is because, especially in fields like literary studies that center the situatedness of human experience, empathy is an ingredient of scholarly analysis – only humans can provide the inherently subjective, nuanced understanding of texts and contexts that are essential to meaningfully interpret, say, a novel about childhood friendships or a painting depicting terminal illness.
Blei explains how topic modeling can uncover hidden thematic structures within large collections of texts and thus offers scholars a tool to analyze documents at a scale that would be unfeasible manually. And Boyd-Graber et al. show how versatile topic models can be in uncovering hidden patterns in literature, data, historical documents, social media, etc. But the use of topic modeling as a form of exploratory research would be incomplete without granular and contextualized analysis. Froehlich’s examination of Moby Dick through word counting is a great example of how simple computational methods can uncover new insights into classic texts – but only if we are engaged with their symbolic meanings.
It was also cool to read about how Antoniak applied topic modeling to her dataset of birth stories to argue that there is “no one “right way” to give birth.” Her approach uncovered recurrent themes and shared experiences within these stories. I found the power differentials particularly interesting, although I’m curious about how she computed these and what an “annotated dictionary of verbs” exactly entails. (edited) 


## Emanuelle Sippy
  8:42 AM
In “Applications of Topic Models,” the comparisons in chapter six were particularly interesting to me, including the contrast between social media needing to be analyzed in the form of longer documents and novels and longer texts that require “segmentation” (70) as well as accounting for languages that do not capitalize and languages in which capitalization indicates other things (72). Moreover, I thought that the authors pointing to the ways that “imaginary worlds” and “character names” shift the ways digital analysis can be used was useful. I am also very interested in the challenge poetry poses for topic modeling, as seen in Rhody’s work. The example about poets using night to talk about death was a particularly strong example of this in my mind. I am curious to learn more about the ways topic models can account for the metaphorical and subtle nature of poetry, especially because writing and analyzing poems has been a huge focus of my undergraduate work personally. Additionally, I appreciated the authors recognition of the limitations of topic modeling. They write: “While topic models provide users with overviews of corpora, topic models cannot be much help if the users cannot effectively see or understand the underlying topics and how they relate to specific documents” (47). I am interested in learning about the best pedagogies for learning how to use topic models with an awareness of the limitations and possibilities they open up in line with this assertion.

## Raphaela Gold
  7:40 AM
(For Thursday the 21): In the Boyd-Graber et al reading, I was excited to see the focus on journalism because I think it is a fascinating field through which to view the digitial humanities, and I’m surprised it hasn’t come up more until this point. I also really appreciate that the authors recognize that readers who already have expertise in topic modeling might not learn a great deal technically, but can still expand their thinking in terms of application. This shows that the writers are considering their audience and trying to make their writing interesting to people with a range of experience using these tools.
	It was interesting to consider that topic models might provide users with a general overview of large bodies of work, but if the users cannot effectively see or understand the underlying topics or how they relate to the documents, these models will not be very effective. Thus, one of my main takeaways was the importance of looking critically at each part of a topic model to determine which aspects are trustworthy and allow users to understand the information they are being given.
	I was definitely most drawn to chapter 6 due to its focus on fiction and literature, as well as applications of topic models in artistic fields more generally. Boyd-Graber et. al touched on the tension between the traditional close reading approach and distant reading, and I thought the authors captured the difference between the two very successfully on page 68, when they wrote, “standard close reading methods require narrow focus and thorough interpretation. Topic models complement close reading in two ways, as a survey method and as a means for tracing and comparing large-scale patterns.” Rather than seeing close and distant reading as diametrically opposed, these authors present them as complementary – or rather, distant reading as complementary to the more traditional method of close reading. To me, this is further evidence that debates over whether to use close or distant reading are unhelpful, because the most effective method is to use both in harmony with one another. Each provides information and tools that the other cannot, and when taken together, a researcher can reach a more well-rounded and nuanced conclusion than if they boxed themselves into one method. I am coming more and more to see these digital humanities tools not as replacements for what came before, but rather as new and exciting options which do not eclipse previous options, but rather add to the corpus.
	The section on “People and Place” in chapter 6 also reminded me of the dataset we looked at and I’m not sure I quite understand what the issue is with a topic model focusing on proper nouns, because character and place can tell a person a lot about a work of fiction, especially if one is already familiar with that fiction through close reading and now wants to zoom out to get a sense of the more general picture. For example, if I’d already read David Copperfield but wanted to see which place he spent the most time in throughout the book, topic modeling which focuses on proper names might be a very effective way of doing that.
	Overall, I am pretty sold on the notion that “Topic models cannot by themselves study literature, but they are useful tools for scholars studying literature” (78). In that sense, a topic model is not much different from going through a book with a highlighter, only it happens more quickly and perhaps less subjectively, depending on the model.
	I found the Antoniak “Computational Reading of a Birth Stories Community” to be a very interesting and somewhat surprising example of how topic modeling can be implemented. Antoniak’s approach of first sharing what got her interested in the topic before sharing the dataset was very compelling because it made me feel personally invested in understanding the results of the dataset. I knew why Antoniak found these birth stories so powerful and narratively complex, and that made the dataset itself more powerful because it might actually help people better understand the traumatic aspects of birth through narrative.
	Antoniak’s implementation of topic modeling felt very well suited to the narrative structure of a typical story, as she focused on topics more likely to occur at the “beginning, middle, and end of the stories.” However, I do wonder how this method might be applied to stories that follow a less typical narrative structure than a birth story recounted on reddit – like an epic, or a story out of chronological order, or a stream of consciousness narrative like Infinite Jest. Additionally, I really enjoyed reading through the dataset, but did find the color scale a bit difficult to make sense of. Still, I admired Antoniak’s approach and how she used topic modeling to discover patterns that had not previously been identified.